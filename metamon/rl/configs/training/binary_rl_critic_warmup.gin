import amago.agent

# Critic warmup uses same reward multiplier as full training
agent.Agent.reward_multiplier = 10.
agent.MultiTaskAgent.reward_multiplier = 10.

agent.Agent.tau = .004
agent.MultiTaskAgent.tau = .004

agent.Agent.num_actions_for_value_in_critic_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_critic_loss = 3

agent.Agent.num_actions_for_value_in_actor_loss = 1
agent.MultiTaskAgent.num_actions_for_value_in_actor_loss = 3

agent.Agent.online_coeff = 0.0
agent.MultiTaskAgent.online_coeff = 0.0
# Critic warmup: Set offline_coeff to 0 to disable actor loss
# This lets critic learn good value estimates before actor training begins
agent.Agent.offline_coeff = 0.0
agent.MultiTaskAgent.offline_coeff = 0.0
agent.Agent.fbc_filter_func = @agent.binary_filter
agent.MultiTaskAgent.fbc_filter_func = @agent.binary_filter

MetamonAMAGOExperiment.l2_coeff = 1e-4
# Use full hyperparameters (same as binary_rl.gin)
MetamonAMAGOExperiment.learning_rate = 1.5e-4
MetamonAMAGOExperiment.grad_clip = 1.5
MetamonAMAGOExperiment.critic_loss_weight = 10.
MetamonAMAGOExperiment.lr_warmup_steps = 1000

# Note: After critic warmup (e.g., 2-5 epochs), switch to binary_rl.gin
# to enable actor training with offline_coeff = 1.0
